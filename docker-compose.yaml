# docker-compose.yaml (v4 - Final)
#
# This version combines the clean .env configuration with the robust
# stability settings (healthchecks, cache volumes) from the original file.
# It represents the best of both worlds.

services:
  mailhog:
    image: mailhog/mailhog
    container_name: mailhog
    ports:
      - "${ALERT_SMTP_PORT:-1025}:1025"
      - "${ALERT_SMTP_WEB_PORT:-8025}:8025"
    networks:
      - defense_network
    restart: unless-stopped

  mock_external_api:
    image: stoplight/prism:4
    container_name: mock_external_api
    command: mock -h 0.0.0.0 /mocks/external_api.yaml
    volumes:
      - ./mocks/external_api.yaml:/mocks/external_api.yaml:ro
    ports:
      - "4010:4010"
    networks:
      - defense_network
    restart: unless-stopped

  nginx_proxy:
    image: openresty/openresty:1.21.4.1-alpine
    container_name: nginx_proxy
    user: nobody
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${NGINX_HTTP_PORT:-80}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/templates/nginx.conf.template:ro
      - ./nginx/lua:/etc/nginx/lua:ro
      - ./nginx/certs:/etc/nginx/certs:ro
      - ./nginx/.htpasswd:/etc/nginx/secrets/.htpasswd:ro
      - ./waf:/etc/nginx/modsecurity:ro
      - ./secrets:/run/secrets:ro
      # Restored Nginx cache volumes for stability and performance
      - nginx_client_body_temp:/var/run/openresty/nginx-client-body
      - nginx_proxy_temp:/var/cache/nginx/proxy_temp
      - nginx_fastcgi_temp:/var/cache/nginx/fastcgi_temp
      - nginx_uwsgi_temp:/var/cache/nginx/uwsgi_temp
      - nginx_scgi_temp:/var/cache/nginx/scgi_temp
      - nginx_logs:/var/log/nginx
    environment:
      - LUA_PATH=/usr/local/openresty/lualib/?.lua;;
      - AI_SERVICE_HOST=ai_service
      - ESCALATION_ENGINE_HOST=escalation_engine
      - TARPIT_API_HOST=tarpit_api
      - ADMIN_UI_HOST=admin_ui
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PASSWORD_FILE=${REDIS_PASSWORD_FILE}
      - ENABLE_HTTPS=${ENABLE_HTTPS}
      - TLS_CERT_PATH=${TLS_CERT_PATH}
      - TLS_KEY_PATH=${TLS_KEY_PATH}
    depends_on:
      escalation_engine:
        condition: service_healthy
      tarpit_api:
        condition: service_healthy
      admin_ui:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped

  apache_proxy:
    image: httpd:2.4
    container_name: apache_proxy
    ports:
      - "${APACHE_HTTP_PORT:-8080}:80"
    volumes:
      - ./apache/httpd.conf:/usr/local/apache2/conf/httpd.conf:ro
    depends_on:
      escalation_engine:
        condition: service_healthy
      tarpit_api:
        condition: service_healthy
      admin_ui:
        condition: service_healthy
      ai_service:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped


  ai_service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai_service
    command: ["uvicorn", "src.ai_service.ai_webhook:app", "--host", "0.0.0.0", "--port", "${AI_SERVICE_PORT:-8000}"]
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./secrets:/run/secrets:ro
    depends_on:
      - escalation_engine
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${AI_SERVICE_PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  escalation_engine:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: escalation_engine
    entrypoint: ["/app/docker-entrypoint.sh"]
    command: ["uvicorn", "src.escalation.escalation_engine:app", "--host", "0.0.0.0", "--port", "${ESCALATION_ENGINE_PORT:-8003}"]
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./models:/app/models
      - ./data:/app/data
      - ./scripts/linux/docker-entrypoint.sh:/app/docker-entrypoint.sh
      - ./secrets:/run/secrets:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${ESCALATION_ENGINE_PORT:-8003}/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  tarpit_api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: tarpit_api
    command: ["uvicorn", "src.tarpit.tarpit_api:app", "--host", "0.0.0.0", "--port", "${TARPIT_API_PORT:-8001}"]
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./archives:/app/archives
      - ./secrets:/run/secrets:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${TARPIT_API_PORT:-8001}/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  admin_ui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: admin_ui
    command: ["python", "src/admin_ui/admin_ui.py"]
    env_file:
      - .env
    ports:
      - "${ADMIN_UI_PORT:-5002}:5002"
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./secrets:/run/secrets:ro
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${ADMIN_UI_PORT:-5002}/"]
      interval: 30s
      timeout: 10s
      retries: 3

  captcha_service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: captcha_service
    command: ["uvicorn", "src.captcha.custom_captcha_service:app", "--host", "0.0.0.0", "--port", "8004"]
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./secrets:/run/secrets:ro
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/verify?token=test&ip=127.0.0.1" ]
      interval: 30s
      timeout: 10s
      retries: 3

  cloud_dashboard:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cloud_dashboard
    command: ["python", "src/cloud_dashboard/cloud_dashboard_api.py"]
    env_file:
      - .env
    ports:
      - "${CLOUD_DASHBOARD_PORT:-5006}:5006"
    volumes:
      - ./src:/app/src
      - ./secrets:/run/secrets:ro
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${CLOUD_DASHBOARD_PORT:-5006}/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  config_recommender:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: config_recommender
    command: ["python", "src/config_recommender/recommender_api.py"]
    env_file:
      - .env
    ports:
      - "8010:8010"
    volumes:
      - ./src:/app/src
      - ./secrets:/run/secrets:ro
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/recommendations" ]
      interval: 30s
      timeout: 10s
      retries: 3

  cloud_proxy:
    build:
      context: ./cloud-proxy
      dockerfile: Dockerfile
    container_name: cloud_proxy
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8008"]
    env_file:
      - .env
    ports:
      - "${CLOUD_PROXY_PORT:-8008}:8008"
    volumes:
      - ./cloud-proxy:/app
      - ./secrets:/run/secrets:ro
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  prompt_router:
    build:
      context: ./prompt-router
      dockerfile: Dockerfile
    container_name: prompt_router
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "${PROMPT_ROUTER_PORT:-8009}"]
    env_file:
      - .env
    ports:
      - "${PROMPT_ROUTER_PORT:-8009}:8009"
    volumes:
      - ./prompt-router:/app
      - ./secrets:/run/secrets:ro
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PROMPT_ROUTER_PORT:-8009}/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  postgres:
    image: postgres:15-alpine
    container_name: postgres_markov_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db/init_markov.sql:/docker-entrypoint-initdb.d/init_markov.sql
      - ./secrets:/run/secrets:ro
    environment:
      # The official Postgres image expects the POSTGRES_* variables, but
      # our .env uses the newer PG_* naming. Map them here so both the
      # container and the rest of the stack get the correct values.
      - POSTGRES_USER=${PG_USER}
      - POSTGRES_PASSWORD_FILE=${PG_PASSWORD_FILE}
      - POSTGRES_DB=${PG_DBNAME}
    ports:
      - "${PG_PORT:-5432}:5432"
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: redis_store
    # Load the password from the same file used by the other services
    command: ["sh", "-c", "redis-server --requirepass $(cat ${REDIS_PASSWORD_FILE})"]
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
      - ./secrets:/run/secrets:ro
    environment:
      - REDIS_PASSWORD_FILE=${REDIS_PASSWORD_FILE}
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a $(cat ${REDIS_PASSWORD_FILE}) ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  blocklist_sync:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: blocklist_sync
    command: ["python", "-m", "scripts.blocklist_sync_daemon"]
    env_file:
      - .env
    volumes:
      - ./secrets:/run/secrets:ro
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped

  fail2ban:
    image: crazymax/fail2ban:latest
    container_name: fail2ban
    network_mode: "host"
    cap_add:
      - NET_ADMIN
      - NET_RAW
    volumes:
      - ./fail2ban:/data
      - nginx_logs:/var/log/nginx:ro
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      - nginx_proxy

  suricata:
    image: jasonish/suricata:latest
    container_name: suricata
    network_mode: "host"
    cap_add:
      - NET_ADMIN
      - NET_RAW
    volumes:
      - ./suricata:/etc/suricata:ro
      - suricata_logs:/var/log/suricata
    command: ["-c", "/etc/suricata/suricata.yaml", "-i", "${SURICATA_INTERFACE:-eth0}"]
    restart: unless-stopped
    depends_on:
      - nginx_proxy

  traefik:
    image: traefik:v2.11
    container_name: traefik
    command:
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --api.dashboard=true
      - --api.insecure=false
    ports:
      - "8005:80"
      - "8081:8080"
    networks:
      - defense_network
    restart: unless-stopped

  llama3:
    image: ollama/ollama:latest
    container_name: llama3
    command: ["sh", "-c", "ollama pull llama3 && ollama serve"]
    ports:
      - "11434:11434"
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.llm.rule=PathPrefix(`/`)"
      - "traefik.http.routers.llm.entrypoints=web"
      - "traefik.http.services.llm.loadbalancer.server.port=11434"
      - "traefik.http.services.llm.loadbalancer.weight=2"
    volumes:
      - ./models/shared-data:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - defense_network
    restart: unless-stopped

  mixtral:
    image: ollama/ollama:latest
    container_name: mixtral
    command: ["sh", "-c", "ollama pull mixtral && ollama serve"]
    ports:
      - "11435:11434"
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.llm.rule=PathPrefix(`/`)"
      - "traefik.http.routers.llm.entrypoints=web"
      - "traefik.http.services.llm.loadbalancer.server.port=11434"
      - "traefik.http.services.llm.loadbalancer.weight=1"
    volumes:
      - ./models/shared-data:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - defense_network
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    networks:
      - defense_network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - defense_network
    depends_on:
      - prometheus
    restart: unless-stopped

  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: --interval ${WATCHTOWER_INTERVAL:-60} --cleanup
    networks:
      - defense_network
    restart: always

networks:
  defense_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  nginx_client_body_temp:
  nginx_proxy_temp:
  nginx_fastcgi_temp:
  nginx_uwsgi_temp:
  nginx_scgi_temp:
  nginx_logs:
  grafana_data:
  suricata_logs:
