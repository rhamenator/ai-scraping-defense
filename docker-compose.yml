services:
  mailhog:
    image: mailhog/mailhog
    container_name: mailhog
    ports:
      - "1025:1025"  # SMTP port
      - "8025:8025"  # Web UI
    networks:
      - defense_network
    restart: unless-stopped

  mock_external_api:
    image: stoplight/prism:4
    container_name: mock_external_api
    command: mock -h 0.0.0.0 /mocks/external_api.yaml
    volumes:
      - ./mocks/external_api.yaml:/mocks/external_api.yaml:ro
    ports:
      - "8000:8000" # Host port:Container port
    networks:
      - defense_network
    restart: unless-stopped

  mock_ip_reputation_api:
    image: stoplight/prism:4
    container_name: mock_ip_reputation_api
    command: mock -h 0.0.0.0 /mocks/ip_reputation_api.yaml
    volumes:
      - ./mocks/ip_reputation_api.yaml:/mocks/ip_reputation_api.yaml:ro
    ports:
      - "8001:8000" # Host port:Container port (Note: mock runs on 8000 in container)
    networks:
      - defense_network
    restart: unless-stopped

  mock_community_blocklist_api:
    image: stoplight/prism:4
    container_name: mock_community_blocklist_api
    command: mock -h 0.0.0.0 /mocks/community_blocklist_api.yaml
    volumes:
      - ./mocks/community_blocklist_api.yaml:/mocks/community_blocklist_api.yaml:ro
    ports:
      - "8002:8000" # Host port:Container port
    networks:
      - defense_network
    restart: unless-stopped

  # --- Main Application Services ---

  nginx:
    build:
      context: .
      dockerfile: Dockerfile # This Dockerfile should build an image with OpenResty/Nginx
    container_name: nginx_proxy
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/lua:/etc/nginx/lua:ro
      - ./config/robots.txt:/etc/nginx/robots.txt:ro # For Nginx to serve if configured & for Lua in Docker
      - ./docs:/var/www/html/docs:ro
      - ./archives:/usr/share/nginx/html/archives:ro # Path Nginx serves archives from
      - ./logs/nginx:/var/log/nginx
      - ./certs:/etc/nginx/certs:ro # For SSL
      # --- ADDED FOR ADMIN UI AUTHENTICATION ---
      - ./secrets/.htpasswd:/etc/nginx/secrets/.htpasswd:ro # Mount the htpasswd file
      # --- END OF ADDED ADMIN UI AUTHENTICATION ---
    depends_on:
      tarpit_api:
        condition: service_healthy # Example: wait for healthcheck
      escalation_engine:
        condition: service_healthy
      admin_ui:
        condition: service_healthy
      ai_service:
        condition: service_healthy
      redis:
        condition: service_started # Or service_healthy if healthcheck added
      postgres:
        condition: service_healthy
    environment: # For Lua scripts if they read from env
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB_BLOCKLIST=2
      - REAL_BACKEND_HOST=${REAL_BACKEND_HOST:-http://your-real-app-service:8080}
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
        test: ["CMD-SHELL", "service nginx status || exit 1"]
        interval: 30s
        timeout: 10s
        retries: 3

  ai_service:
    build:
      context: .
      dockerfile: Dockerfile # All Python services use the same base image
    container_name: ai_service
    working_dir: /app
    command: ["uvicorn", "ai_service.ai_webhook:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "${UVICORN_WORKERS:-2}"]
    environment:
      - PYTHONPATH=/app
      - ALERT_SMTP_HOST=mailhog # Example, should be in .env
      - ALERT_SMTP_PORT=1025   # Example, should be in .env
      # Other env vars loaded from .env file by Docker Compose
    volumes:
      - ./ai_service:/app/ai_service
      - ./shared:/app/shared
      - ./metrics.py:/app/metrics.py:ro
      - ./logs:/app/logs
      - ./data:/app/data
      - ./util:/app/util
    secrets:
      - smtp_password
      - community_blocklist_api_key
      - redis_password
    depends_on:
      redis:
        condition: service_healthy # Changed to service_healthy assuming Redis has one
      mailhog:
        condition: service_started
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  escalation_engine:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: escalation_engine
    working_dir: /app
    command: ["uvicorn", "escalation.escalation_engine:app", "--host", "0.0.0.0", "--port", "8003", "--workers", "${UVICORN_WORKERS:-2}"]
    environment:
      - PYTHONPATH=/app
      # Other env vars loaded from .env
    volumes:
      - ./escalation:/app/escalation
      - ./shared:/app/shared
      - ./metrics.py:/app/metrics.py:ro
      - ./models:/app/models:ro # Models are read-only for this service
      - ./config:/app/config:ro # Configs like robots.txt
      - ./logs:/app/logs
      - ./data:/app/data
      - ./util:/app/util
    secrets:
      - external_api_key
      - ip_reputation_api_key
      - redis_password
      # - smtp_password # Only if escalation engine itself sends SMTP alerts
    depends_on:
      ai_service: # Ensures AI service is up if escalations immediately trigger actions
        condition: service_healthy
      redis:
        condition: service_healthy
      # postgres: # If it directly queries PG, though less likely for escalation logic itself
      #   condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8003/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s

  tarpit_api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: tarpit_api
    working_dir: /app
    command: ["uvicorn", "tarpit.tarpit_api:app", "--host", "0.0.0.0", "--port", "8001", "--workers", "${UVICORN_WORKERS:-2}"]
    environment:
      - PYTHONPATH=/app
      # WIKIPEDIA_CORPUS_FILE is used by corpus_updater, not directly by tarpit_api runtime usually
      # Other env vars loaded from .env
    volumes:
      - ./tarpit:/app/tarpit
      - ./shared:/app/shared
      - ./metrics.py:/app/metrics.py:ro
      - ./logs:/app/logs
      - ./data:/app/data # For Markov model if it were file-based (now PG)
      - ./util:/app/util
    secrets:
      - pg_password # For PostgreSQL Markov DB
      - redis_password # For hop limit and blocklist trigger
      - system_seed # For deterministic tarpit generation
    depends_on:
      escalation_engine:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres: # Tarpit API now relies on PostgreSQL for Markov chains
        condition: service_healthy
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s

  admin_ui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: admin_ui
    working_dir: /app
    command: ["python", "admin_ui/admin_ui.py"] # Flask dev server
    environment:
      - PYTHONPATH=/app
      - FLASK_ENV=${FLASK_ENV:-production}
      # Other env vars from .env
    volumes:
      - ./admin_ui:/app/admin_ui
      - ./shared:/app/shared
      - ./metrics.py:/app/metrics.py:ro # Admin UI reads metrics
      - ./logs:/app/logs # If admin_ui writes its own logs
      # - ./data:/app/data # Unlikely needed by admin_ui directly
      # - ./util:/app/util # Unlikely needed
    networks:
      - defense_network
    restart: unless-stopped
    # No standard healthcheck for Flask dev server, use depends_on if critical path

  archive_rotator:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: archive_rotator
    working_dir: /app
    command: ["python", "tarpit/rotating_archive.py"]
    environment:
      - PYTHONPATH=/app
      - DEFAULT_ARCHIVE_DIR=/app/fake_archives # Path inside container where script writes
    volumes:
      - ./tarpit:/app/tarpit # For rotating_archive.py and js_zip_generator.py
      # - ./shared:/app/shared # If it uses shared modules
      - ./archives:/app/fake_archives # Mount host ./archives to where script writes
      - ./logs:/app/logs # For its own logs
      # - ./util:/app/util # If it uses utilities
    networks:
      - defense_network
    restart: unless-stopped
    # This is a batch-like job, healthcheck might not be typical.
    # Consider if it needs to run periodically (then a cron job in K8s, or host cron for Docker)

  redis:
    image: redis:7-alpine
    container_name: redis_store
    volumes:
      - redis_data:/data
    networks:
      - defense_network
    restart: unless-stopped
    command: >
      sh -c 'if [ -f /run/secrets/redis_password.txt ] && [ -s /run/secrets/redis_password.txt ]; then \
               echo "Starting Redis with password protection."; \
               exec redis-server --save 60 1 --loglevel warning --requirepass "$(cat /run/secrets/redis_password.txt)"; \
             else \
               echo "Starting Redis without password protection (password file not found or empty)."; \
               exec redis-server --save 60 1 --loglevel warning; \
             fi'
    secrets:
      - source: redis_password # This refers to the secret defined at the bottom of docker-compose.yml
        target: /run/secrets/redis_password.txt
        mode: 0400 # Restrict permissions of the secret file in the container
    healthcheck:
      test: >
        sh -c 'if [ -f /run/secrets/redis_password.txt ] && [ -s /run/secrets/redis_password.txt ]; then \
                 redis-cli -a "$(cat /run/secrets/redis_password.txt)" ping | grep -q PONG; \
               else \
                 redis-cli ping | grep -q PONG; \
               fi'
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s # Give Redis a moment to start

  postgres:
    image: postgres:15-alpine
    container_name: postgres_markov_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db/init_markov.sql:/docker-entrypoint-initdb.d/init_markov.sql:ro # For schema creation
    environment:
      - POSTGRES_DB=${PG_DBNAME:-markovdb}
      - POSTGRES_USER=${PG_USER:-markovuser}
      - POSTGRES_PASSWORD_FILE=/run/secrets/pg_password.txt # Tells PG entrypoint script where to find password
    secrets:
      - source: pg_password
        target: /run/secrets/pg_password.txt
        # mode: 0400
    networks:
      - defense_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER:-markovuser} -d $${POSTGRES_DB:-markovdb} -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s # Allow time for init scripts on first run

# Utility/Training Services (can be run on demand)
  corpus_updater: # For dynamic Wikipedia corpus generation
    build:
      context: .
      dockerfile: Dockerfile
    container_name: corpus_updater
    working_dir: /app
    command: ["python", "util/corpus_wikipedia_updater.py"] # Example command
    environment:
      - PYTHONPATH=/app
      - WIKIPEDIA_CORPUS_FILE=/app/data/wikipedia_corpus.txt # Path inside container
      - WIKIPEDIA_NUM_ARTICLES=${WIKIPEDIA_NUM_ARTICLES:-5}
      - WIKIPEDIA_LANGUAGE=${WIKIPEDIA_LANGUAGE:-en}
    volumes:
      - ./util:/app/util
      - ./data:/app/data # Mount host ./data to where script writes corpus
    networks:
      - defense_network
    # This is a one-off or periodic task, so `restart: on-failure` or run manually.
    # Not typically part of `docker-compose up -d` for continuous running.

  robots_fetcher: # For dynamic robots.txt fetching
    build:
      context: .
      dockerfile: Dockerfile
    container_name: robots_fetcher
    working_dir: /app
    command: ["python", "util/robots_fetcher.py"] # Example command
    environment:
      - PYTHONPATH=/app
      - REAL_BACKEND_HOST=${REAL_BACKEND_HOST_FOR_ROBOTS:-http://your-real-app-service:8080} # Needs to be set
      # For Docker Compose, it doesn't update a K8s ConfigMap.
      # It would typically write to a file mounted by Nginx.
      # This service's role is more prominent in K8s. For local testing,
      # Nginx directly mounts ./config/robots.txt
    volumes:
      - ./util:/app/util
      # - ./config:/app/config # If it were to write robots.txt here for Nginx to pick up
    networks:
      - defense_network

  markov_trainer: # For training the PostgreSQL Markov model
    build:
      context: .
      dockerfile: Dockerfile
    container_name: markov_trainer
    working_dir: /app
    # Command will be `python rag/train_markov_postgres.py /app/data/your_corpus.txt`
    # Run this manually: docker-compose run --rm markov_trainer python rag/train_markov_postgres.py /app/data/your_corpus_file.txt
    environment:
      - PYTHONPATH=/app
      # PG_HOST, PG_PORT, PG_DBNAME, PG_USER, PG_PASSWORD_FILE from .env
    volumes:
      - ./rag:/app/rag
      - ./data:/app/data # For corpus input
    secrets:
      - pg_password
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - defense_network

  model_trainer: # For training the RF model
    build:
      context: .
      dockerfile: Dockerfile
    container_name: model_trainer
    working_dir: /app
    # Command: python rag/training.py
    # Run manually: docker-compose run --rm model_trainer python rag/training.py
    environment:
      - PYTHONPATH=/app
      # All TRAINING_* env vars from .env
      # PG_HOST, PG_PORT, TRAINING_PG_DBNAME, TRAINING_PG_USER, TRAINING_PG_PASSWORD_FILE from .env
    volumes:
      - ./rag:/app/rag
      - ./shared:/app/shared # If training uses shared modules
      - ./metrics.py:/app/metrics.py:ro # If training script uses metrics
      - ./models:/app/models # For saving the trained model
      - ./data:/app/data # For input logs, finetuning data output
      - ./config:/app/config # For robots.txt input
      - ./logs:/app/logs # For feedback logs (honeypot, captcha)
    secrets:
      - training_pg_password # Assuming a separate password/secret for this DB if different
      # Or use pg_password if it's the same DB/user
    depends_on:
      postgres: # If it uses the same PG instance for log analysis DB
        condition: service_healthy
    networks:
      - defense_network


networks:
  defense_network:
    driver: bridge

volumes:
  redis_data:
  postgres_data:
  # models_data: # If you want ./models to be a named volume
  # archives_data: # If you want ./archives to be a named volume
  # corpus_data: # If you want ./data (for corpus) to be a named volume

secrets:
  smtp_password:
    file: ./secrets/smtp_password.txt
  external_api_key:
    file: ./secrets/external_api_key.txt
  ip_reputation_api_key:
    file: ./secrets/ip_reputation_api_key.txt
  community_blocklist_api_key:
    file: ./secrets/community_blocklist_api_key.txt
  pg_password: # For Markov DB
    file: ./secrets/pg_password.txt
  redis_password:
    file: ./secrets/redis_password.txt
  system_seed: # For Tarpit API
    file: ./secrets/system_seed.txt # Create this file with your seed
  training_pg_password: # For the log analysis DB used by rag/training.py
    file: ./secrets/training_pg_password.txt # Create if different from main pg_password
